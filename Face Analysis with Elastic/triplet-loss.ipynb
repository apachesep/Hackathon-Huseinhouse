{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread,imresize\n",
    "from random import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import mobilenet_v2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'mobilenet_v2_1.0_224'\n",
    "checkpoint = checkpoint_name + '.ckpt'\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "dir_data = ['husein','negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X, data_Y = [], []\n",
    "for no, i in enumerate(dir_data):\n",
    "    dirs = ['%s/%s'%(i,n) for n in os.listdir(i)]\n",
    "    data_X += dirs\n",
    "    data_Y += [no]*len(dirs)\n",
    "    \n",
    "c = list(zip(data_X, data_Y))\n",
    "shuffle(c)\n",
    "data_X, data_Y = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "    square_norm = tf.diag_part(dot_product)\n",
    "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "        distances = distances + mask * 1e-16\n",
    "        distances = tf.sqrt(distances)\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
    "    assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
    "\n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = tf.to_float(tf.greater(triplet_loss, 1e-16))\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
    "\n",
    "    return triplet_loss, fraction_positive_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from mobilenet_v2_1.0_224.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'huseinhouse/emotion-checkpoint-face.ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "X = tf.placeholder(tf.float32,[None,224,224,3])\n",
    "Y = tf.placeholder(tf.int64, [None])\n",
    "images = X\n",
    "images = tf.map_fn(lambda image: tf.image.per_image_standardization(image), images)\n",
    "with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope(is_training=True)):\n",
    "    logits, endpoints = mobilenet_v2.mobilenet(images,num_classes=20)\n",
    "        \n",
    "cost, fraction = batch_all_triplet_loss(Y, logits, margin=0.5, squared=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "var_lists = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'MobilenetV2')\n",
    "var_lists_without_logits = [i for i in var_lists if i.name.find('Conv2d_1c') < 0]\n",
    "saver = tf.train.Saver(var_list = var_lists_without_logits)\n",
    "saver.restore(sess, checkpoint)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "# test save\n",
    "saver.save(sess, \"huseinhouse/emotion-checkpoint-face.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg cost: 0.174313\n",
      "epoch: 2, avg cost: 0.074930\n",
      "epoch: 3, avg cost: 0.101085\n",
      "epoch: 4, avg cost: 0.010760\n",
      "epoch: 5, avg cost: 0.034187\n",
      "epoch: 6, avg cost: 0.255204\n",
      "epoch: 7, avg cost: 0.083906\n",
      "epoch: 8, avg cost: 0.000000\n",
      "epoch: 9, avg cost: 0.000000\n",
      "epoch: 10, avg cost: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'huseinhouse/emotion-checkpoint-face.ckpt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "for i in range(10):\n",
    "    total_loss = 0\n",
    "    for index in range(0,len(data_X),batch_size):\n",
    "        batch_images = data_X[index:min(index+batch_size,len(data_X))]\n",
    "        batch_x = np.zeros((len(batch_images),224,224,3))\n",
    "        for k in range(len(batch_images)):\n",
    "            batch_x[k] = imresize(imread(data_X[index+k]), (224,224))\n",
    "        batch_y = data_Y[index:min(index+batch_size,len(data_X))]\n",
    "        loss, _ = sess.run([cost,optimizer],feed_dict={X:batch_x,Y:batch_y})\n",
    "        total_loss += loss\n",
    "    total_loss /= len(data_X)\n",
    "    print('epoch: %d, avg cost: %f'%(i+1,total_loss))\n",
    "saver.save(sess, \"huseinhouse/emotion-checkpoint-face.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = []\n",
    "for index in range(0,len(data_X),batch_size):\n",
    "    batch_images = data_X[index:min(index+batch_size,len(data_X))]\n",
    "    batch_x = np.zeros((len(batch_images),224,224,3))\n",
    "    for k in range(len(batch_images)):\n",
    "        batch_x[k] = imresize(imread(data_X[index+k]), (224,224))\n",
    "    embedded.append(logits.eval({X:batch_x}))\n",
    "embedded = np.concatenate(embedded,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-fca70b234907>:10: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "INFO:tensorflow:Restoring parameters from huseinhouse/emotion-checkpoint-face.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "X = tf.placeholder(tf.float32,[None,224,224,3])\n",
    "images = X\n",
    "images = tf.map_fn(lambda image: tf.image.per_image_standardization(image), images)\n",
    "np_embedded = tf.convert_to_tensor(embedded, np.float32)\n",
    "with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope(is_training=True)):\n",
    "    logits, endpoints = mobilenet_v2.mobilenet(images,num_classes=20)\n",
    "        \n",
    "normalized = tf.nn.l2_normalize(logits, dim = 1)\n",
    "prod = tf.matmul(normalized, np_embedded,adjoint_b = True)\n",
    "dist = 1 - prod\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "saver.restore(sess, \"huseinhouse/emotion-checkpoint-face.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64 ms, sys: 4 ms, total: 68 ms\n",
      "Wall time: 65.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted = sess.run(logits,feed_dict={X:np.expand_dims(imresize(imread(data_X[0]), (224,224)),0)})[0]\n",
    "data_Y[np.argmin(cdist(embedded, [predicted], 'cosine')[:,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = np.zeros((batch_size,224,224,3))\n",
    "for k in range(batch_size):\n",
    "    batch_x[k] = imresize(imread(data_X[k]), (224,224))\n",
    "predicted = sess.run(logits,feed_dict={X:batch_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data_Y)[np.argmin(cdist(embedded, predicted, 'cosine'),axis=0)] == data_Y[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
